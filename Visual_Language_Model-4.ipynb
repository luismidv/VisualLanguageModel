{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY90X8aYNafQ"
      },
      "source": [
        "<h2> Image Embedding </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kT1WtA0NR4b",
        "outputId": "2fddb1cb-5d80-4de7-dd20-0dd3f83d23e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install huggingface_hub\n",
        "!pip install tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "class PatchEmbedings(torch.nn.Module):\n",
        "    def __init__(self, img_size = 224, patch_size = 16, hidden_size = 768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        #CONVOLOTUION FOR PATCH EXTRACTION\n",
        "        self.conv = nn.Conv2d(in_channels = 3, out_channels = hidden_size, kernel_size = patch_size, stride = patch_size)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.conv.weight)\n",
        "        if self.conv.bias is not None:\n",
        "            nn.init.zeros_(self.conv.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.size(2) != self.img_size or x.size(3) != self.img_size:\n",
        "            raise ValueError(f\"Input image size is different than model trained one {x.shape}. \\n It must be {self.img_size} x {self.img_size}\")\n",
        "        x = self.conv(x)\n",
        "        x = x.flatten(2) #This way I remain the batches and channels unchanged and since the H&W are now H/patc_size = num_patches\n",
        "        x = x.transpose(1, 2) #NOW THE TENSOR IS (Num_batches, num_patches, hidden_size_channels)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHt9KcZZNhr-"
      },
      "source": [
        "<h2>Multi-Head Self Attention mechanism</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkFboLC7Npj3"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, dropout, is_decoder, n_embd, head_size):\n",
        "        super().__init__(n_embd, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd,head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "    def forward(self,x):\n",
        "        num_batches,seq_length,num_channels = x.shape\n",
        "        key = self.key(x)\n",
        "        query = self.query(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        wei = query @ key.transpose(-2,-1) * (num_channels ** -0.5)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            tril = torch.tril(torch.ones(seq_length, seq_length, dtpye = torch.bool, device = x.device))\n",
        "            wei = wei.masked_fill(tril == 0, float = \"-inf\")\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ value\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0MkUW9cNxce"
      },
      "outputs": [],
      "source": [
        "class MultiModalProjector(nn.Module):\n",
        "    def __init__(self, n_embd, image_embed_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(image_embed_dim, image_embed_dim * 4),\n",
        "            nn.GeLU(),\n",
        "            nn.Linear(image_embed_dim*4, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wipjvUHNz68"
      },
      "outputs": [],
      "source": [
        "class VisionLanguageModel(nn.Module):\n",
        "    def __init__(self, n_embd, image_embed_dim, vocab_size,\n",
        "                 n_layer, img_size, patch_size, num_heads,\n",
        "                 num_blks, emb_dropout, blk_dropout):\n",
        "        super().__init__()\n",
        "        num_hiddens = image_embed_dim\n",
        "        assert num_hiddens % num_heads == 0\n",
        "\n",
        "        self.vision_encoder = PatchEmbedings(96,16,512)\n",
        "        self.decoder = Head(0.1,True,512,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc92YNSROPFF"
      },
      "source": [
        "<h2> Helper functions </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NORFU7RmN3A-"
      },
      "outputs": [],
      "source": [
        "def image_embedding(image,patch_size):\n",
        "    print(f\"Before unfold {image.shape}\")\n",
        "    patches = image.unfold(2,size = patch_size, step = patch_size).unfold(3,size = patch_size, step = patch_size)\n",
        "    num_patches = ()\n",
        "    num_patches_w = image.shape[2]//patch_size\n",
        "    num_patches_h = image.shape[3]//patch_size\n",
        "    num_patches = num_patches_h * num_patches_w\n",
        "    print(f\"After unfold {patches.shape}\")\n",
        "    #TODO .CONTIGUOUS IS NECCESARRY FOR .VIEW\n",
        "    patches = patches.permute(0,2,3,1,4,5).contiguous()\n",
        "    print(patches.shape)\n",
        "    patches = patches.view(image.shape[0], num_patches, -1)\n",
        "    print(f\"Patches shape: {patches.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mgk6rt3OUKa"
      },
      "source": [
        "<h2> Main code </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOjJ4dUgD8-E"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,features,labels):\n",
        "    #TRAINING EXAMPLES\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "    #TRANSFORMS\n",
        "    self.train_transforms = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.RandomHorizontalFlip(p = 0.4),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean = [0.5,0.5,0.5], std = [0.5, 0.5 ,0.5])\n",
        "    ])\n",
        "\n",
        "    #TOKENZATION\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    self.embedding = nn.Embedding(self.tokenizer.vocab_size,768)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "    features_iter = self.features[idx]\n",
        "    labels_iter = self.labels[idx]\n",
        "\n",
        "    label_token = self.tokenizer(labels_iter,padding = \"max_length\", max_length=14, truncation = True, return_tensors = \"pt\")\n",
        "    feature_transform = self.train_transforms(features_iter)\n",
        "\n",
        "    label_tokens = self.embedding(label_token['input_ids'])\n",
        "    label_tokens_final = label_tokens.unsqueeze(0)\n",
        "\n",
        "    attention_mask = label_token['attention_mask']\n",
        "    attention_mask = self.embedding(attention_mask).unsqueeze(0)\n",
        "\n",
        "    return feature_transform,label_tokens_final, attention_mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return feature_transform, label_tokens_final\n",
        "\n",
        "\n",
        "def model_training(dataloader, head):\n",
        "\n",
        "  for (x,y,z) in tqdm(dataloader, desc = \"TRAINING\"):\n",
        "    x = x.to(\"cpu\")\n",
        "    y = x.to(\"cpu\")\n",
        "    z = z.to(\"cpu\")\n",
        "    print(f\"X shape {x.shape}\")\n",
        "    prediction = head(x)\n",
        "    print(prediction.shape)\n",
        "\n",
        "def tokenizer_trials(tokens):\n",
        "  max_length = 0\n",
        "  for dict in tokens:\n",
        "    for sentences in dict['tokens']:\n",
        "      if len(sentences) > max_length:\n",
        "        max_length = len(sentences)\n",
        "\n",
        "  print(max_length)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkcF8hzNcbds",
        "outputId": "ca5965aa-f66f-4041-cf26-25a032430d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['caption', 'image'],\n",
            "    num_rows: 102512\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "login(token=\"\")\n",
        "dataset = load_dataset(\"xcpan/coco2017\", split =\"train\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "s6PjLuJgOOKu",
        "outputId": "fed7ecc9-0f72-4771-bbdf-4ac810eec88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   6%|▋         | 1/16 [00:00<00:05,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  12%|█▎        | 2/16 [00:00<00:04,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  19%|█▉        | 3/16 [00:01<00:04,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  25%|██▌       | 4/16 [00:01<00:04,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  31%|███▏      | 5/16 [00:01<00:03,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  38%|███▊      | 6/16 [00:02<00:03,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n",
            "X shape torch.Size([32, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  44%|████▍     | 7/16 [00:02<00:03,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 196, 768])\n",
            "X shape torch.Size([32, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  50%|█████     | 8/16 [00:03<00:03,  2.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 196, 768])\n",
            "X shape torch.Size([32, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  56%|█████▋    | 9/16 [00:03<00:03,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 196, 768])\n",
            "X shape torch.Size([32, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  62%|██████▎   | 10/16 [00:04<00:02,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 196, 768])\n",
            "X shape torch.Size([32, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  69%|██████▉   | 11/16 [00:04<00:02,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  75%|███████▌  | 12/16 [00:04<00:01,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  81%|████████▏ | 13/16 [00:05<00:01,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  88%|████████▊ | 14/16 [00:05<00:00,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTRAINING:  94%|█████████▍| 15/16 [00:05<00:00,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING: 100%|██████████| 16/16 [00:06<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape torch.Size([20, 3, 224, 224])\n",
            "torch.Size([20, 196, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-6bf78560b4ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtraining_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mfeature_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "small_dataset = dataset.shuffle(seed=42).select(range(500))\n",
        "patch = PatchEmbedings()\n",
        "#TODO USE THIS TO TRAIN WITH A LOWER VERSION OF THE DATASET\n",
        "  #dataset = load_dataset(\"HuggingFaceM4/COCO\", split = \"train\")\n",
        "\n",
        "images = small_dataset['image']\n",
        "caption = small_dataset['caption']\n",
        "#images, conversations = data_checking(images, conversations)\n",
        "\n",
        "#new_tokens = [token['tokens'] for token in tokens]\n",
        "dataset_class = Dataset(images, caption)\n",
        "training_loader = DataLoader(dataset_class, batch_size = 32, shuffle=True)\n",
        "\n",
        "feature_transform, final_tokens, attention_mask = model_training(training_loader, patch)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}